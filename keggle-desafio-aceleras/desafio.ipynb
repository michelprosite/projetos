{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michel Souza Santana\n",
    "## Projeto Desafio Aceleras\n",
    "## Trilha 1\n",
    "> Start: 15/05/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o pyrrow para conversão dos arquivos csv em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os diretórios estruturis do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório já existe.\n"
     ]
    }
   ],
   "source": [
    "diretorio = \"/opt/projetos/keggle-desafio-aceleras\" \n",
    "\n",
    "if not os.path.exists(diretorio + '/engineer'):\n",
    "    os.makedirs(diretorio + 'engineer')\n",
    "    print(\"Diretório criado com sucesso!\")\n",
    "else:\n",
    "    print(\"O diretório já existe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório já existe.\n"
     ]
    }
   ],
   "source": [
    "diretorio = \"/opt/projetos/keggle-desafio-aceleras\" \n",
    "\n",
    "if not os.path.exists(diretorio + '/raw'):\n",
    "    os.makedirs(diretorio + 'engineer')\n",
    "    print(\"Diretório criado com sucesso!\")\n",
    "else:\n",
    "    print(\"O diretório já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório já existe.\n"
     ]
    }
   ],
   "source": [
    "diretorio = \"/opt/projetos/keggle-desafio-aceleras\" \n",
    "\n",
    "if not os.path.exists(diretorio + '/refined'):\n",
    "    os.makedirs(diretorio + 'engineer')\n",
    "    print(\"Diretório criado com sucesso!\")\n",
    "else:\n",
    "    print(\"O diretório já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório já existe.\n"
     ]
    }
   ],
   "source": [
    "diretorio = \"/opt/projetos/keggle-desafio-aceleras\" \n",
    "\n",
    "if not os.path.exists(diretorio + '/transient'):\n",
    "    os.makedirs(diretorio + 'engineer')\n",
    "    print(\"Diretório criado com sucesso!\")\n",
    "else:\n",
    "    print(\"O diretório já existe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório já existe.\n"
     ]
    }
   ],
   "source": [
    "diretorio = \"/opt/projetos/keggle-desafio-aceleras\" \n",
    "\n",
    "if not os.path.exists(diretorio + '/trusted'):\n",
    "    os.makedirs(diretorio + 'engineer')\n",
    "    print(\"Diretório criado com sucesso!\")\n",
    "else:\n",
    "    print(\"O diretório já existe.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o arquivo 'controller.csv' na pasta enginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo CSV já existe.\n"
     ]
    }
   ],
   "source": [
    "# Caminho do arquivo CSV\n",
    "caminho_arquivo = '/opt/projetos/keggle-desafio-aceleras/engineer/controller.csv'\n",
    "dados = ''\n",
    "\n",
    "# Verificar se já existe o arquivo, se não, abrir o arquivo CSV em modo de escrita e criá-lo\n",
    "if not os.path.exists(caminho_arquivo):\n",
    "    with open(caminho_arquivo, 'w', newline='') as arquivo_csv:\n",
    "        writer = csv.writer(arquivo_csv)\n",
    "        writer.writerows(dados)\n",
    "\n",
    "    print(\"Arquivo CSV criado com sucesso.\")\n",
    "else:\n",
    "    print(\"Arquivo CSV já existe.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificar as fontes de dados: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "\n",
    "A URL em questão se refere a um conjunto de dados disponibilizado no site Kaggle, que contém informações sobre o comércio eletrônico no Brasil. O conjunto de dados é intitulado \"Brazilian E-Commerce Public Dataset by Olist\" e foi criado pela empresa Olist, que é uma plataforma de vendas on-line que conecta pequenos e médios varejistas a marketplaces.\n",
    "\n",
    "O conjunto de dados contém informações de mais de 100 mil pedidos de clientes, com dados que incluem informações do produto, preços, prazos de entrega, avaliações de clientes e informações sobre o vendedor. Além disso, o conjunto de dados contém informações sobre geolocalização dos clientes, categoria de produtos e informações sobre a própria loja virtual.\n",
    "\n",
    "Este conjunto de dados pode ser extremamente útil para análises sobre comércio eletrônico no Brasil, permitindo a análise de tendências de consumo, comportamento dos clientes, performance de vendas e muito mais. A disponibilização de dados desse tipo é importante para o desenvolvimento de modelos de negócios mais eficientes e para a tomada de decisões mais informadas no setor de e-commerce brasileiro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copia os dados das fontes do kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando o Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando os arquivos csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo já existe.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('/opt/projetos/keggle-desafio-aceleras/transient/kaggle_csv/brazilian-ecommerce.zip'):\n",
    "    !cd /opt/projetos/keggle-desafio-aceleras/transient/kaggle_csv/ && kaggle datasets download -d olistbr/brazilian-ecommerce\n",
    "    !cd /opt/projetos/keggle-desafio-aceleras/transient/kaggle_csv/ && unzip brazilian-ecommerce.zip\n",
    "    print('Arquivo carregado e descompactado.')\n",
    "else:\n",
    "    print('Arquivo já existe.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as variáveis necessárias para manipulação dos arquivos csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path's das pastas\n",
    "path_transient = '/opt/projetos/keggle-desafio-aceleras/transient'\n",
    "path_raw = '/opt/projetos/keggle-desafio-aceleras/raw'\n",
    "\n",
    "# Path's dos arquivos\n",
    "path_customers = path_transient + '/kaggle_csv/olist_customers_dataset.csv'\n",
    "path_geolocation = path_transient + '/kaggle_csv/olist_geolocation_dataset.csv'\n",
    "path_order_items = path_transient + '/kaggle_csv/olist_order_items_dataset.csv'\n",
    "path_order_payments = path_transient + '/kaggle_csv/olist_order_payments_dataset.csv'\n",
    "path_order_reviews = path_transient + '/kaggle_csv/olist_order_reviews_dataset.csv'\n",
    "path_orders = path_transient + '/kaggle_csv/olist_orders_dataset.csv'\n",
    "path_products = path_transient + '/kaggle_csv/olist_products_dataset.csv'\n",
    "path_sellers = path_transient + '/kaggle_csv/olist_sellers_dataset.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id                 object\n",
      "customer_unique_id          object\n",
      "customer_zip_code_prefix    object\n",
      "customer_city               object\n",
      "customer_state              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_customers = pd.read_csv(path_customers)\n",
    "df_customers = df_customers.astype('str')\n",
    "df_customers.to_parquet(path_raw + \"/customers.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_customers.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geolocation_zip_code_prefix    object\n",
      "geolocation_lat                object\n",
      "geolocation_lng                object\n",
      "geolocation_city               object\n",
      "geolocation_state              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_geolocation = pd.read_csv(path_geolocation)\n",
    "df_geolocation = df_geolocation.astype('str')\n",
    "df_geolocation.to_parquet(path_raw + \"/geolocation.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_geolocation.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id               object\n",
      "order_item_id          object\n",
      "product_id             object\n",
      "seller_id              object\n",
      "shipping_limit_date    object\n",
      "price                  object\n",
      "freight_value          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_order_items = pd.read_csv(path_order_items)\n",
    "df_order_items = df_order_items.astype('str')\n",
    "df_order_items.to_parquet(path_raw + \"/order_items.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_order_items.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id                object\n",
      "payment_sequential      object\n",
      "payment_type            object\n",
      "payment_installments    object\n",
      "payment_value           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_order_payments = pd.read_csv(path_order_payments)\n",
    "df_order_payments = df_order_payments.astype('str')\n",
    "df_order_payments.to_parquet(path_raw + \"/order_payments.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_order_payments.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id                  object\n",
      "order_id                   object\n",
      "review_score               object\n",
      "review_comment_title       object\n",
      "review_comment_message     object\n",
      "review_creation_date       object\n",
      "review_answer_timestamp    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_order_reviews = pd.read_csv(path_order_reviews)\n",
    "df_order_reviews = df_order_reviews.astype('str')\n",
    "df_order_reviews.to_parquet(path_raw + \"/order_reviews.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_order_reviews.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id                         object\n",
      "customer_id                      object\n",
      "order_status                     object\n",
      "order_purchase_timestamp         object\n",
      "order_approved_at                object\n",
      "order_delivered_carrier_date     object\n",
      "order_delivered_customer_date    object\n",
      "order_estimated_delivery_date    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_orders = pd.read_csv(path_orders)\n",
    "df_orders = df_orders.astype('str')\n",
    "df_orders.to_parquet(path_raw + \"/orders.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_orders.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_id                    object\n",
      "product_category_name         object\n",
      "product_name_lenght           object\n",
      "product_description_lenght    object\n",
      "product_photos_qty            object\n",
      "product_weight_g              object\n",
      "product_length_cm             object\n",
      "product_height_cm             object\n",
      "product_width_cm              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_products = pd.read_csv(path_products)\n",
    "df_products = df_products.astype('str')\n",
    "df_products.to_parquet(path_raw + \"/products.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_products.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seller_id                 object\n",
      "seller_zip_code_prefix    object\n",
      "seller_city               object\n",
      "seller_state              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_sellers = pd.read_csv(path_sellers)\n",
    "df_sellers = df_sellers.astype('str')\n",
    "df_sellers.to_parquet(path_raw + \"/sellers.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_sellers.dtypes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma tabela via conexõ com Mysql existente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instalando mysql connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-8.0.33-cp310-cp310-manylinux1_x86_64.whl (27.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/27.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf<=3.20.3,>=3.11.0 in /usr/lib/python3/dist-packages (from mysql-connector-python) (3.12.4)\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-8.0.33\n"
     ]
    }
   ],
   "source": [
    "#!pip install mysql-connector-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "password = '*Mss140920@'\n",
    "database = 'olist_db'\n",
    "\n",
    "conn = mysql.connector.connect(host=host, user=user, password=password, database=database)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "create_table_query = '''\n",
    "CREATE TABLE employees (\n",
    "  id INT PRIMARY KEY AUTO_INCREMENT,\n",
    "  name VARCHAR(100) NOT NULL,\n",
    "  age INT,\n",
    "  department VARCHAR(50)\n",
    ")\n",
    "'''\n",
    "\n",
    "cursor.execute(create_table_query)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
