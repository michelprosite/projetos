{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Michel Souza Santana\n",
    "## Projeto Desafio Aceleras\n",
    "## Trilha 1\n",
    "> Start: 15/05/2023"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1 - Transformação do ER proposto em um BI, realizando o ETL usando uma ferrmenta local (Talend, Apache Hop, Nifi, Airflow, SSIS, Pentaho,…)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Entenda o modelo ER: Familiarize-se com o modelo ER existente, incluindo as tabelas, relacionamentos e atributos. Isso ajudará você a mapear corretamente os dados durante a transformação.\n",
    "\n",
    "* Identifique os requisitos de BI: Compreenda as necessidades e requisitos do seu projeto de BI. Identifique as informações que você precisa extrair e apresentar no ambiente de BI.\n",
    "\n",
    "* Escolha uma ferramenta ETL: Pesquise e selecione uma ferramenta ETL adequada para sua transformação de dados. Existem várias opções disponíveis, como Pentaho Data Integration, Talend, Microsoft SQL Server Integration Services (SSIS), entre outras.\n",
    "\n",
    "* Instale a ferramenta ETL: Faça o download e instale a ferramenta ETL selecionada no seu ambiente local.\n",
    "\n",
    "* Conecte-se ao banco de dados: Configure a conexão da ferramenta ETL com o banco de dados que contém os dados do modelo ER. Forneça as credenciais de acesso necessárias para estabelecer a conexão.\n",
    "\n",
    "* Extração de dados: Utilizando a ferramenta ETL, extraia os dados do banco de dados conforme necessário para o seu projeto de BI. Isso pode envolver a seleção de tabelas específicas, filtragem de dados ou até mesmo a união de várias tabelas para obter as informações desejadas.\n",
    "\n",
    "* Limpeza e transformação de dados: Aplique as transformações necessárias nos dados extraídos para adequá-los às necessidades do ambiente de BI. Isso pode incluir a remoção de dados duplicados, preenchimento de valores ausentes, conversão de formatos de data, entre outros processos de limpeza e transformação.\n",
    "\n",
    "* Mapeamento para o modelo dimensional: Se você estiver construindo um data warehouse ou uma solução de BI baseada em modelo dimensional, mapeie os dados extraídos para as dimensões e fatos do seu modelo dimensional. Isso envolve a definição de hierarquias, chaves e relacionamentos.\n",
    "\n",
    "* Desenvolva fluxos de trabalho ETL: Utilizando a ferramenta ETL, crie fluxos de trabalho que automatizem a transformação de dados. Isso pode envolver a criação de transformações, tarefas agendadas e outras operações para garantir a integridade e atualização dos dados.\n",
    "\n",
    "* Carregamento dos dados: Carregue os dados transformados no ambiente de BI, que pode incluir um data warehouse, um banco de dados ou outra solução de armazenamento de dados.\n",
    "\n",
    "* Desenvolva visualizações e relatórios: Com os dados carregados no ambiente de BI, desenvolva visualizações e relatórios interativos para fornecer insights acionáveis aos usuários finais. Isso pode ser feito usando ferramentas de visualização de dados como Tableau, Power BI, QlikView, entre outras.\n",
    "\n",
    "* Teste e valide: Realize testes para garantir a precisão e a integridade dos dados transformados. Verifique se as visualiza"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagrama Entidade Relacionamento"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entidades:\n",
    "- Cliente (Customer)\n",
    "- Pedido (Order)\n",
    "- Item do Pedido (Order Item)\n",
    "- Produto (Product)\n",
    "- Vendedor (Seller)\n",
    "- Categoria de Produto (Product Category)\n",
    "- Avaliação (Review)\n",
    "- Localização Geográfica (Geolocation)\n",
    "\n",
    "Relacionamentos:\n",
    "- O Cliente (Customer) pode fazer vários Pedidos (Order)\n",
    "- Um Pedido (Order) pertence a um único Cliente (Customer)\n",
    "- Um Pedido (Order) possui vários Itens do Pedido (Order Item)\n",
    "- Cada Item do Pedido (Order Item) está associado a um único Produto (Product)\n",
    "- Um Produto (Product) pertence a uma Categoria de Produto (Product Category)\n",
    "- Um Produto (Product) é vendido por um Vendedor (Seller)\n",
    "- Um Pedido (Order) pode ter várias Avaliações (Review)\n",
    "- Cada Avaliação (Review) está associada a um único Pedido (Order)\n",
    "- Cada Vendedor (Seller) está associado a uma Localização Geográfica (Geolocation)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](../Tabela-relacionamneto-olist.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* PAYMENT:\n",
    "    * pk_payment_order_id (chave primária)\n",
    "    * fk_order_id (chave estrangeira referenciando ORDERS)\n",
    "    * payment_sequential\n",
    "    * payment_type\n",
    "    * payment_installments\n",
    "    * payment_value\n",
    "\n",
    "****\n",
    "\n",
    "* PRODUCT:\n",
    "    * pk_product_id (chave primária)\n",
    "    * product_category\n",
    "    * product_name\n",
    "    \n",
    "****\n",
    "\n",
    "* REVIEW:\n",
    "    * pk_review_id (chave primária)\n",
    "    * fk_order_id (chave estrangeira referenciando ORDERS)\n",
    "    * review_score\n",
    "    * review_comment_title\n",
    "    * review_comment_message\n",
    "    * review_creation_date\n",
    "    * review_answer_timestamp\n",
    "    \n",
    "****\n",
    "\n",
    "* ORDERS:\n",
    "    * pk_order_id (chave primária)\n",
    "    * fk_payment_order_id (chave estrangeira referenciando PAYMENT)\n",
    "    * fk_customer_id (chave estrangeira referenciando CUSTOMER)\n",
    "    * order_status\n",
    "    * order_purchase_timestamp\n",
    "    * order_approved_at\n",
    "    * order_delivered_carrier_date\n",
    "    * order_delivered_customer_date\n",
    "    * order_estimated_delivery_date\n",
    "    \n",
    "****\n",
    "\n",
    "* ORDER_ITEM:\n",
    "    * pk_order_item_id (chave primária)\n",
    "    * fk_order_id (chave estrangeira referenciando ORDERS)\n",
    "    * fk_product_id (chave estrangeira referenciando PRODUCT)\n",
    "    * fk_seller_id (chave estrangeira referenciando SELLERS)\n",
    "    * shipping_limit_date\n",
    "    * price\n",
    "    * freight_value\n",
    "    \n",
    "****\n",
    "\n",
    "* SELLERS:\n",
    "    * pk_seller_id (chave primária)\n",
    "    * fk_geolocation_zip_code_prefix (chave estrangeira referenciando GEOLOCATION)\n",
    "    * seller_city\n",
    "    * seller_state\n",
    "    \n",
    "****\n",
    "\n",
    "* CUSTOMER:\n",
    "    * pk_customer_id (chave primária)\n",
    "    * fk_zip_code_prefix (chave estrangeira referenciando GEOLOCATION)\n",
    "    * customer_unique_id\n",
    "    * customer_city\n",
    "    * customer_state\n",
    "    \n",
    "****\n",
    "\n",
    "* GEOLOCATION:\n",
    "    * pk_geolocation_zip_code_prefix (chave primária)\n",
    "    * geolocation_lat\n",
    "    * geolocation_lng\n",
    "    * geolocation_city\n",
    "    * geolocation_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregando as bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalando o pyrrow para conversão dos arquivos csv em parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyarrow\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando os diretórios estruturais do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.read_csv('/opt/projetos/keggle-desafio-aceleras/controller_folders.csv')\n",
    "lista_folders = f['Folders'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O diretório engineer já existe.\n",
      "O diretório raw já existe.\n",
      "O diretório refined já existe.\n",
      "O diretório transient já existe.\n",
      "O diretório trusted já existe.\n"
     ]
    }
   ],
   "source": [
    "for i in lista_folders:\n",
    "    diretorio = \"/opt/projetos/keggle-desafio-aceleras\" \n",
    "\n",
    "    if not os.path.exists(diretorio + '/' + i):\n",
    "        os.makedirs(diretorio + '/' + i)\n",
    "        print(f\"Diretório {i} criado com sucesso!\")\n",
    "    else:\n",
    "        print(f\"O diretório {i} já existe.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o arquivo 'controller.csv' na pasta enginner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo CSV já existe.\n"
     ]
    }
   ],
   "source": [
    "# Caminho do arquivo CSV\n",
    "caminho_arquivo = '/opt/projetos/keggle-desafio-aceleras/engineer/controller.csv'\n",
    "dados = ''\n",
    "\n",
    "# Verificar se já existe o arquivo, se não, abrir o arquivo CSV em modo de escrita e criá-lo\n",
    "if not os.path.exists(caminho_arquivo):\n",
    "    with open(caminho_arquivo, 'w', newline='') as arquivo_csv:\n",
    "        writer = csv.writer(arquivo_csv)\n",
    "        writer.writerows(dados)\n",
    "\n",
    "    print(\"Arquivo CSV criado com sucesso.\")\n",
    "else:\n",
    "    print(\"Arquivo CSV já existe.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificar as fontes de dados: "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\n",
    "\n",
    "A URL em questão se refere a um conjunto de dados disponibilizado no site Kaggle, que contém informações sobre o comércio eletrônico no Brasil. O conjunto de dados é intitulado \"Brazilian E-Commerce Public Dataset by Olist\" e foi criado pela empresa Olist, que é uma plataforma de vendas on-line que conecta pequenos e médios varejistas a marketplaces.\n",
    "\n",
    "O conjunto de dados contém informações de mais de 100 mil pedidos de clientes, com dados que incluem informações do produto, preços, prazos de entrega, avaliações de clientes e informações sobre o vendedor. Além disso, o conjunto de dados contém informações sobre geolocalização dos clientes, categoria de produtos e informações sobre a própria loja virtual.\n",
    "\n",
    "Este conjunto de dados pode ser extremamente útil para análises sobre comércio eletrônico no Brasil, permitindo a análise de tendências de consumo, comportamento dos clientes, performance de vendas e muito mais. A disponibilização de dados desse tipo é importante para o desenvolvimento de modelos de negócios mais eficientes e para a tomada de decisões mais informadas no setor de e-commerce brasileiro."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copia os dados das fontes do kaggle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instalando o Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install kaggle --upgrade"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baixando os arquivos csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo já existe.\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('/opt/projetos/keggle-desafio-aceleras/transient/brazilian-ecommerce.zip'):\n",
    "    !cd /opt/projetos/keggle-desafio-aceleras/transient/ && kaggle datasets download -d olistbr/brazilian-ecommerce\n",
    "    !cd /opt/projetos/keggle-desafio-aceleras/transient/ && unzip brazilian-ecommerce.zip\n",
    "    print('Arquivo carregado e descompactado.')\n",
    "else:\n",
    "    print('Arquivo já existe.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as variáveis necessárias para manipulação dos arquivos csv's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path's das pastas\n",
    "path_transient = '/opt/projetos/keggle-desafio-aceleras/transient'\n",
    "path_raw = '/opt/projetos/keggle-desafio-aceleras/raw'\n",
    "\n",
    "# Path's dos arquivos\n",
    "path_customers = path_transient + '/olist_customers_dataset.csv'\n",
    "path_geolocation = path_transient + '/olist_geolocation_dataset.csv'\n",
    "path_order_items = path_transient + '/olist_order_items_dataset.csv'\n",
    "path_order_payments = path_transient + '/olist_order_payments_dataset.csv'\n",
    "path_order_reviews = path_transient + '/olist_order_reviews_dataset.csv'\n",
    "path_orders = path_transient + '/olist_orders_dataset.csv'\n",
    "path_products = path_transient + '/olist_products_dataset.csv'\n",
    "path_sellers = path_transient + '/olist_sellers_dataset.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_id                 object\n",
      "customer_unique_id          object\n",
      "customer_zip_code_prefix    object\n",
      "customer_city               object\n",
      "customer_state              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_customers = pd.read_csv(path_customers)\n",
    "df_customers = df_customers.astype('str')\n",
    "df_customers.to_parquet(path_raw + \"/customers.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_customers.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geolocation_zip_code_prefix    object\n",
      "geolocation_lat                object\n",
      "geolocation_lng                object\n",
      "geolocation_city               object\n",
      "geolocation_state              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_geolocation = pd.read_csv(path_geolocation)\n",
    "df_geolocation = df_geolocation.astype('str')\n",
    "df_geolocation.to_parquet(path_raw + \"/geolocation.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_geolocation.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id               object\n",
      "order_item_id          object\n",
      "product_id             object\n",
      "seller_id              object\n",
      "shipping_limit_date    object\n",
      "price                  object\n",
      "freight_value          object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_order_items = pd.read_csv(path_order_items)\n",
    "df_order_items = df_order_items.astype('str')\n",
    "df_order_items.to_parquet(path_raw + \"/order_items.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_order_items.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id                object\n",
      "payment_sequential      object\n",
      "payment_type            object\n",
      "payment_installments    object\n",
      "payment_value           object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_order_payments = pd.read_csv(path_order_payments)\n",
    "df_order_payments = df_order_payments.astype('str')\n",
    "df_order_payments.to_parquet(path_raw + \"/order_payments.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_order_payments.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review_id                  object\n",
      "order_id                   object\n",
      "review_score               object\n",
      "review_comment_title       object\n",
      "review_comment_message     object\n",
      "review_creation_date       object\n",
      "review_answer_timestamp    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_order_reviews = pd.read_csv(path_order_reviews)\n",
    "df_order_reviews = df_order_reviews.astype('str')\n",
    "df_order_reviews.to_parquet(path_raw + \"/order_reviews.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_order_reviews.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_id                         object\n",
      "customer_id                      object\n",
      "order_status                     object\n",
      "order_purchase_timestamp         object\n",
      "order_approved_at                object\n",
      "order_delivered_carrier_date     object\n",
      "order_delivered_customer_date    object\n",
      "order_estimated_delivery_date    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_orders = pd.read_csv(path_orders)\n",
    "df_orders = df_orders.astype('str')\n",
    "df_orders.to_parquet(path_raw + \"/orders.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_orders.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_id                    object\n",
      "product_category_name         object\n",
      "product_name_lenght           object\n",
      "product_description_lenght    object\n",
      "product_photos_qty            object\n",
      "product_weight_g              object\n",
      "product_length_cm             object\n",
      "product_height_cm             object\n",
      "product_width_cm              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_products = pd.read_csv(path_products)\n",
    "df_products = df_products.astype('str')\n",
    "df_products.to_parquet(path_raw + \"/products.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_products.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seller_id                 object\n",
      "seller_zip_code_prefix    object\n",
      "seller_city               object\n",
      "seller_state              object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_sellers = pd.read_csv(path_sellers)\n",
    "df_sellers = df_sellers.astype('str')\n",
    "df_sellers.to_parquet(path_raw + \"/sellers.parquet\")\n",
    "\n",
    "# Verificar os tipos de dados convertidos\n",
    "print(df_sellers.dtypes)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando uma tabela via conexão com Mysql existente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### instalando mysql connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install mysql-connector-python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelagem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](Captura%20de%20tela%20de%202023-05-15%2021-17-16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['customer_id',\n",
       " 'customer_unique_id',\n",
       " 'customer_zip_code_prefix',\n",
       " 'customer_city',\n",
       " 'customer_state']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = list(df_customers.columns)\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['geolocation_zip_code_prefix',\n",
       " 'geolocation_lat',\n",
       " 'geolocation_lng',\n",
       " 'geolocation_city',\n",
       " 'geolocation_state']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = list(df_geolocation.columns)\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id',\n",
       " 'customer_id',\n",
       " 'order_status',\n",
       " 'order_purchase_timestamp',\n",
       " 'order_approved_at',\n",
       " 'order_delivered_carrier_date',\n",
       " 'order_delivered_customer_date',\n",
       " 'order_estimated_delivery_date']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = list(df_orders.columns)\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id',\n",
       " 'order_item_id',\n",
       " 'product_id',\n",
       " 'seller_id',\n",
       " 'shipping_limit_date',\n",
       " 'price',\n",
       " 'freight_value']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = list(df_order_items.columns)\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>product_id</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>shipping_limit_date</th>\n",
       "      <th>price</th>\n",
       "      <th>freight_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00010242fe8c5a6d1ba2dd792cb16214</td>\n",
       "      <td>1</td>\n",
       "      <td>4244733e06e7ecb4970a6e2683c13e61</td>\n",
       "      <td>48436dade18ac8b2bce089ec2a041202</td>\n",
       "      <td>2017-09-19 09:45:35</td>\n",
       "      <td>58.9</td>\n",
       "      <td>13.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00018f77f2f0320c557190d7a144bdd3</td>\n",
       "      <td>1</td>\n",
       "      <td>e5f2d52b802189ee658865ca93d83a8f</td>\n",
       "      <td>dd7ddc04e1b6c2c614352b383efe2d36</td>\n",
       "      <td>2017-05-03 11:05:13</td>\n",
       "      <td>239.9</td>\n",
       "      <td>19.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000229ec398224ef6ca0657da4fc703e</td>\n",
       "      <td>1</td>\n",
       "      <td>c777355d18b72b67abbeef9df44fd0fd</td>\n",
       "      <td>5b51032eddd242adc84c38acab88f23d</td>\n",
       "      <td>2018-01-18 14:48:30</td>\n",
       "      <td>199.0</td>\n",
       "      <td>17.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00024acbcdf0a6daa1e931b038114c75</td>\n",
       "      <td>1</td>\n",
       "      <td>7634da152a4610f1595efa32f14722fc</td>\n",
       "      <td>9d7a1d34a5052409006425275ba1c2b4</td>\n",
       "      <td>2018-08-15 10:10:18</td>\n",
       "      <td>12.99</td>\n",
       "      <td>12.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00042b26cf59d7ce69dfabb4e55b4fd9</td>\n",
       "      <td>1</td>\n",
       "      <td>ac6c3623068f30de03045865e4e10089</td>\n",
       "      <td>df560393f3a51e74553ab94004ba5c87</td>\n",
       "      <td>2017-02-13 13:57:51</td>\n",
       "      <td>199.9</td>\n",
       "      <td>18.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id order_item_id   \n",
       "0  00010242fe8c5a6d1ba2dd792cb16214             1  \\\n",
       "1  00018f77f2f0320c557190d7a144bdd3             1   \n",
       "2  000229ec398224ef6ca0657da4fc703e             1   \n",
       "3  00024acbcdf0a6daa1e931b038114c75             1   \n",
       "4  00042b26cf59d7ce69dfabb4e55b4fd9             1   \n",
       "\n",
       "                         product_id                         seller_id   \n",
       "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202  \\\n",
       "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36   \n",
       "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d   \n",
       "3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   \n",
       "4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87   \n",
       "\n",
       "   shipping_limit_date  price freight_value  \n",
       "0  2017-09-19 09:45:35   58.9         13.29  \n",
       "1  2017-05-03 11:05:13  239.9         19.93  \n",
       "2  2018-01-18 14:48:30  199.0         17.87  \n",
       "3  2018-08-15 10:10:18  12.99         12.79  \n",
       "4  2017-02-13 13:57:51  199.9         18.14  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_order_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['order_id',\n",
       " 'payment_sequential',\n",
       " 'payment_type',\n",
       " 'payment_installments',\n",
       " 'payment_value']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = list(df_order_payments.columns)\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review_id',\n",
       " 'order_id',\n",
       " 'review_score',\n",
       " 'review_comment_title',\n",
       " 'review_comment_message',\n",
       " 'review_creation_date',\n",
       " 'review_answer_timestamp']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = list(df_order_reviews.columns)\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['product_id',\n",
       " 'product_category_name',\n",
       " 'product_name_lenght',\n",
       " 'product_description_lenght',\n",
       " 'product_photos_qty',\n",
       " 'product_weight_g',\n",
       " 'product_length_cm',\n",
       " 'product_height_cm',\n",
       " 'product_width_cm']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = list(df_products.columns)\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['product_id',\n",
       " 'product_category_name',\n",
       " 'product_name_lenght',\n",
       " 'product_description_lenght',\n",
       " 'product_photos_qty',\n",
       " 'product_weight_g',\n",
       " 'product_length_cm',\n",
       " 'product_height_cm',\n",
       " 'product_width_cm']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = list(df_products.columns)\n",
    "lista\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['seller_id', 'seller_zip_code_prefix', 'seller_city', 'seller_state']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = list(df_sellers.columns)\n",
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Conectando ao banco de dados MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='*Mss140920@',\n",
    "    database='olist_db'\n",
    ")\n",
    "\n",
    "# Cursor para executar as consultas SQL\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Lista de queries SQL para criação das tabelas\n",
    "create_table_queries = [\n",
    "    \"\"\"\n",
    "    -- Criação da tabela GEOLOCATION\n",
    "    CREATE TABLE IF NOT EXISTS GEOLOCATION (\n",
    "      pk_geolocation_zip_code_prefix INT PRIMARY KEY,\n",
    "      geolocation_lat DECIMAL(10, 6),\n",
    "      geolocation_lng DECIMAL(10, 6),\n",
    "      geolocation_city VARCHAR(50),\n",
    "      geolocation_state VARCHAR(50)\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Criação da tabela CUSTOMER\n",
    "    CREATE TABLE IF NOT EXISTS CUSTOMER (\n",
    "      pk_customer_id INT PRIMARY KEY,\n",
    "      fk_zip_code_prefix INT,\n",
    "      customer_unique_id VARCHAR(50),\n",
    "      customer_city VARCHAR(50),\n",
    "      customer_state VARCHAR(50),\n",
    "      FOREIGN KEY (fk_zip_code_prefix) REFERENCES GEOLOCATION(pk_geolocation_zip_code_prefix)\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Criação da tabela SELLERS\n",
    "    CREATE TABLE IF NOT EXISTS SELLERS (\n",
    "      pk_seller_id INT PRIMARY KEY,\n",
    "      fk_geolocation_zip_code_prefix INT,\n",
    "      seller_city VARCHAR(50),\n",
    "      seller_state VARCHAR(50),\n",
    "      FOREIGN KEY (fk_geolocation_zip_code_prefix) REFERENCES GEOLOCATION(pk_geolocation_zip_code_prefix)\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Criação da tabela PRODUCT\n",
    "    CREATE TABLE IF NOT EXISTS PRODUCT (\n",
    "      pk_product_id INT PRIMARY KEY,\n",
    "      product_category VARCHAR(50),\n",
    "      product_name VARCHAR(50)\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Criação da tabela ORDERS\n",
    "    CREATE TABLE IF NOT EXISTS ORDERS (\n",
    "      pk_order_id INT PRIMARY KEY,\n",
    "      fk_payment_order_id INT,\n",
    "      fk_customer_id INT,\n",
    "      order_status VARCHAR(50),\n",
    "      order_purchase_timestamp DATE,\n",
    "      order_approved_at DATE,\n",
    "      order_delivered_carrier_date DATE,\n",
    "      order_delivered_customer_date DATE,\n",
    "      order_estimated_delivery_date DATE\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Criação da tabela REVIEW\n",
    "    CREATE TABLE IF NOT EXISTS REVIEW (\n",
    "      pk_review_id INT PRIMARY KEY,\n",
    "      fk_order_id INT,\n",
    "      review_score INT,\n",
    "      review_comment_title VARCHAR(50),\n",
    "      review_comment_message VARCHAR(200),\n",
    "      review_creation_date DATE,\n",
    "      review_answer_timestamp DATE,\n",
    "      FOREIGN KEY (fk_order_id) REFERENCES ORDERS(pk_order_id)\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Criação da tabela PAYMENT\n",
    "    CREATE TABLE IF NOT EXISTS PAYMENT (\n",
    "      pk_payment_order_id INT PRIMARY KEY,\n",
    "      fk_order_id INT,\n",
    "      payment_sequential INT,\n",
    "      payment_type VARCHAR(50),\n",
    "      payment_installments INT,\n",
    "      payment_value DECIMAL(10, 2)\n",
    "    )\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Adição da chave estrangeira para a tabela PAYMENT referenciando a tabela ORDERS\n",
    "    ALTER TABLE PAYMENT\n",
    "    ADD FOREIGN KEY (fk_order_id) REFERENCES ORDERS(pk_order_id)\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Executar as consultas SQL para criar as tabelas\n",
    "for query in create_table_queries:\n",
    "    cursor.execute(query)\n",
    "\n",
    "# Confirmar as alterações no banco de dados\n",
    "cnx.commit()\n",
    "\n",
    "# Fechar a conexão com o banco de dados\n",
    "cursor.close()\n",
    "cnx.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "# Conectando ao banco de dados MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password='*Mss140920@',\n",
    "    database='olist_db'\n",
    ")\n",
    "\n",
    "# Cursor para executar as consultas SQL\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Adicionar tabela ORDER_ITEM\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS ORDER_ITEM (\n",
    "        pk_order_item_id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        fk_order_id INT,\n",
    "        fk_product_id INT,\n",
    "        fk_seller_id INT,\n",
    "        shipping_limit_date DATETIME,\n",
    "        price DECIMAL(10, 2),\n",
    "        freight_value DECIMAL(10, 2),\n",
    "        FOREIGN KEY (fk_order_id) REFERENCES ORDERS(pk_order_id),\n",
    "        FOREIGN KEY (fk_product_id) REFERENCES PRODUCT(pk_product_id),\n",
    "        FOREIGN KEY (fk_seller_id) REFERENCES SELLERS(pk_seller_id)\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Adicionando relações de chave estrangeira nas tabelas existentes\n",
    "alter_table_queries = [\n",
    "    \"\"\"\n",
    "    -- Adicionando relação de chave estrangeira na tabela PAYMENT\n",
    "    ALTER TABLE PAYMENT\n",
    "    ADD FOREIGN KEY (fk_order_id) REFERENCES ORDERS(pk_order_id)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Adicionando relação de chave estrangeira na tabela REVIEW\n",
    "    ALTER TABLE REVIEW\n",
    "    ADD FOREIGN KEY (fk_order_id) REFERENCES ORDERS(pk_order_id)\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    -- Adicionando relação de chave estrangeira na tabela ORDER_ITEM\n",
    "    ALTER TABLE ORDER_ITEM\n",
    "    ADD FOREIGN KEY (fk_order_id) REFERENCES ORDERS(pk_order_id),\n",
    "    ADD FOREIGN KEY (fk_product_id) REFERENCES PRODUCT(pk_product_id),\n",
    "    ADD FOREIGN KEY (fk_seller_id) REFERENCES SELLERS(pk_seller_id)\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "# Executar as consultas SQL para adicionar as relações de chave estrangeira\n",
    "for query in alter_table_queries:\n",
    "    cursor.execute(query)\n",
    "\n",
    "# Confirmar as alterações no banco de dados\n",
    "cnx.commit()\n",
    "\n",
    "# Fechar a conexão com o banco de dados\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Conectando ao banco de dados MySQL\n",
    "cnx = mysql.connector.connect(\n",
    "    host='localhost',\n",
    "    user='seu_usuario',\n",
    "    password='sua_senha',\n",
    "    database='seu_banco_de_dados'\n",
    ")\n",
    "\n",
    "# Ler os dados do arquivo CSV usando o pandas\n",
    "dados_csv = pd.read_csv('caminho_do_arquivo.csv')\n",
    "\n",
    "# Nome da tabela no banco de dados\n",
    "nome_tabela = 'nome_da_tabela'\n",
    "\n",
    "# Colunas da tabela no banco de dados (devem corresponder às colunas do CSV)\n",
    "colunas_tabela = ['coluna1', 'coluna2', 'coluna3']\n",
    "\n",
    "# Cursor para executar as consultas SQL\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# Inserir os dados na tabela\n",
    "for _, linha in dados_csv.iterrows():\n",
    "    valores = [linha[coluna] for coluna in colunas_tabela]\n",
    "    query = f\"INSERT INTO {nome_tabela} ({', '.join(colunas_tabela)}) VALUES ({', '.join(['%s']*len(colunas_tabela))})\"\n",
    "    cursor.execute(query, valores)\n",
    "\n",
    "# Confirmar as alterações no banco de dados\n",
    "cnx.commit()\n",
    "\n",
    "# Fechar a conexão com o banco de dados\n",
    "cursor.close()\n",
    "cnx.close()\"\"\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2 - Enviar esse BI para a Nuvem (Azure / GCP), pensando em uma cloud e desenvolvendo um pipeline fim a fim, entregando o DW em uma camada final para visualização."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Defina a estratégia de nuvem: Avalie as necessidades do seu projeto e decida qual plataforma em nuvem (Azure ou GCP) atende melhor aos seus requisitos. Considere fatores como escalabilidade, disponibilidade de serviços e integração com suas ferramentas e tecnologias existentes.\n",
    "\n",
    "* Provisione recursos na nuvem: Crie uma conta na plataforma escolhida (Azure ou GCP) e provisione os recursos necessários. Isso pode incluir máquinas virtuais, armazenamento, bancos de dados, serviços de data warehouse, entre outros.\n",
    "\n",
    "* Migre o data warehouse para a nuvem: Realize a migração dos dados do seu data warehouse local para o ambiente em nuvem. Isso pode ser feito através de exportação/importação de dados ou replicação contínua, dependendo da sua estratégia e tamanho do conjunto de dados.\n",
    "\n",
    "* Configure o ambiente de data warehouse na nuvem: Configure o ambiente de data warehouse na plataforma em nuvem, criando esquemas, tabelas e índices conforme necessário. Garanta que a estrutura do data warehouse na nuvem esteja alinhada com o modelo dimensional que você definiu.\n",
    "\n",
    "* Desenvolva o pipeline de dados: Utilize as ferramentas disponíveis na plataforma em nuvem para criar um pipeline de dados automatizado. Isso pode incluir serviços como Azure Data Factory (Azure) ou Cloud Dataflow (GCP). Defina as etapas de extração, transformação e carga (ETL) dos dados do seu data warehouse local para o data warehouse na nuvem.\n",
    "\n",
    "* Agende o pipeline de dados: Configure agendamentos regulares para o pipeline de dados, garantindo que as atualizações do data warehouse local sejam refletidas no ambiente em nuvem. Considere a frequência necessária para manter os dados atualizados e sincronizados.\n",
    "\n",
    "* Implemente segurança e conformidade: Aplique as práticas recomendadas de segurança e conformidade na nuvem, como controle de acesso, criptografia e monitoramento de auditoria. Certifique-se de que os dados e o ambiente de BI estejam protegidos contra ameaças e em conformidade com as regulamentações aplicáveis.\n",
    "\n",
    "* Escolha uma ferramenta de visualização de dados: Selecione uma ferramenta de visualização de dados adequada à plataforma em nuvem escolhida, como Power BI (Azure) ou Data Studio (GCP). Essas ferramentas permitem criar painéis interativos, relatórios e gráficos com base nos dados armazenados no data warehouse na nuvem.\n",
    "\n",
    "* Desenvolva visualizações e relatórios: Utilize a ferramenta de visualização de dados escolhida para criar visualizações e relatórios interativos. Explore recursos como filtros, drill-downs e painéis personalizados para fornecer insights acionáveis aos usuários finais.\n",
    "\n",
    "* Publique o BI na nuvem: Faça o deploy do seu ambiente de BI na nuvem para que os usuários possam acessá-lo. Isso pode ser feito através de compartilhamento de links, integração com portais corporativos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3 - Apontar alternativas viáveis de evolução da solução proposta e apresentar um desenho da arquitetura da proposta final"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Avalie os requisitos atuais e futuros: Entenda os requisitos atuais da solução proposta e identifique possíveis necessidades futuras. Considere o crescimento esperado do volume de dados, a demanda por recursos computacionais e quaisquer outros requisitos específicos.\n",
    "\n",
    "* Pesquise as tecnologias disponíveis: Realize uma pesquisa das tecnologias e soluções disponíveis no mercado que possam atender às necessidades identificadas. Verifique as tendências de mercado e as melhores práticas para obter insights sobre as opções mais recentes.\n",
    "\n",
    "* Identifique as alternativas viáveis: Com base na pesquisa realizada, identifique as alternativas viáveis de evolução da solução proposta. Isso pode incluir a adoção de novas ferramentas de ETL, o uso de serviços de data lake, a implementação de data streaming, a integração de ferramentas de inteligência artificial ou aprendizado de máquina, entre outras opções.\n",
    "\n",
    "* Analise os prós e contras de cada alternativa: Avalie os prós e contras de cada alternativa identificada. Considere aspectos como escalabilidade, desempenho, custo, complexidade de implementação, recursos disponíveis na plataforma em nuvem escolhida, compatibilidade com a solução atual e habilidades da equipe.\n",
    "\n",
    "* Escolha a alternativa mais adequada: Com base na análise realizada, escolha a alternativa que seja mais adequada para atender aos requisitos atuais e futuros da solução proposta. Leve em consideração fatores como custo-benefício, viabilidade técnica e alinhamento estratégico com a organização.\n",
    "\n",
    "* Desenhe a arquitetura proposta: Com a alternativa selecionada, elabore um desenho da arquitetura da proposta final. Isso envolve identificar os componentes principais, as integrações entre eles e o fluxo de dados. Considere a separação de camadas (por exemplo, ingestão, processamento, armazenamento, visualização) e a escalabilidade horizontal ou vertical.\n",
    "\n",
    "* Especifique os componentes da arquitetura: Para cada componente da arquitetura proposta, especifique as tecnologias e serviços específicos a serem utilizados. Por exemplo, se você optar por um data lake, especifique a ferramenta de armazenamento, como Azure Data Lake Storage ou Google Cloud Storage. Se escolher um serviço de ETL, identifique a ferramenta específica.\n",
    "\n",
    "* Considere a segurança e a governança: Certifique-se de incluir aspectos de segurança e governança na arquitetura proposta. Isso pode envolver a definição de políticas de acesso aos dados, a implementação de criptografia, a garantia da conformidade com regulamentações de privacidade de dados e a criação de um framework de governança para a solução.\n",
    "\n",
    "* Documente a arquitetura proposta: Documente todos os detalhes da arquitetura proposta, incluindo os componentes, integrações, fluxo de dados, tecnologias e serviços utilizados. Isso ajudará na comunicação com as partes interessadas e no entendimento da"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
